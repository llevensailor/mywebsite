[
  {
    "objectID": "mapping.html#introduction",
    "href": "mapping.html#introduction",
    "title": "Air Pollution",
    "section": "Introduction",
    "text": "Introduction\nAir pollution is a public health concern that countries all around the world need to take into consideration. Each country has a different way of reporting air pollution levels. According to Wikipedia, the United States Environmental Protection Agency (EPA) created the Air Quality Index (AQI) that is based on the five pollutants: ground-level ozone, particulate matter, carbon monoxide, sulfur dioxide, and nitrogen dioxide. The six levels of public health concerns are pictured below.\n\n\n\nAir Quality Index\n\n\nSeveral datasets were used in this project.\n\nThe dataset containing AQI scores for the United States can be found here\nThe dataset containing AQI scores for the rest of the world can be found here"
  },
  {
    "objectID": "mapping.html#world-aqi-scores",
    "href": "mapping.html#world-aqi-scores",
    "title": "Air Pollution",
    "section": "World AQI Scores",
    "text": "World AQI Scores"
  },
  {
    "objectID": "mapping.html#aqi-scores-in-asia-oceania",
    "href": "mapping.html#aqi-scores-in-asia-oceania",
    "title": "Air Pollution",
    "section": "AQI Scores in Asia & Oceania",
    "text": "AQI Scores in Asia & Oceania\nReferencing map is Figure 1\n\n\n\n\n\n\n\n\nFigure 1: Map of AQI Scores in Asia"
  },
  {
    "objectID": "mapping.html#aqi-scores-in-europe",
    "href": "mapping.html#aqi-scores-in-europe",
    "title": "Air Pollution",
    "section": "AQI Scores in Europe",
    "text": "AQI Scores in Europe\nReferencing map in text is Figure 2\n\n\n\n\n\n\n\n\nFigure 2: Map of AQI Scores in Europe"
  },
  {
    "objectID": "mapping.html#aqi-scores-in-north-and-south-america",
    "href": "mapping.html#aqi-scores-in-north-and-south-america",
    "title": "Air Pollution",
    "section": "AQI Scores in North and South America",
    "text": "AQI Scores in North and South America\n\n\n\n\n\n\n\n\nFigure 3: Map of AQI Scores in North and South America\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Map of AQI Scores in the United States\n\n\n\n\n\nNorth and South America were difficult to make the plots for because the United States was not included in the AQI data for all of the countries in the world that we found. To fix this issue Leah decided to create a separate figure (Figure 4) for the United States, separated by the average AQI of each state. Looking at the individual states first, the trends seem to be the more south west you go, the worse the AQI gets. California has the worst AQI in the United States on average which is due to the constant wildfires and the overpopulation which causes high vehicular emissions. On the flip side, Alaska has one of the best AQI scores on average partially due to the less densely populated areas. The entire country as a whole hovers around the 35 area. Now looking at the countries in North and South America the scale has changed. The range for the AQI for the United States is roughly 20-55, but the range for the America continents is roughly 25-200 so on the national range, the United States would have a relatively low AQI. All of the countries are within the 50-100 range except Chile which has a score of over 200. According to Un Dispatch, the reason for this is because the majority of people in Chile heat their homes via wood burning which creates a lot of air pollution."
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "WCA_data/WAC_README.html",
    "href": "WCA_data/WAC_README.html",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "Encoding: UTF-8\nDate: March 12, 2025\nExport Format Version: 1.0.0\nContact: WCA Results Team (https://www.worldcubeassociation.org/contact?contactRecipient=wrt)\nWebsite: https://www.worldcubeassociation.org/export/results\n\n\n\nThis database export contains public information about all official WCA competitions, WCA members, and WCA competition results.\n\n\n\nThe goal of this database export is to provide members of the speedcubing community a practical way to perform analysis on competition information for statistical and personal purposes.\n\n\n\nThe information in this file may be re-published, in whole or in part, as long as users are clearly notified of the following:\n\nThis information is based on competition results owned and maintained by the World Cube Assocation, published at https://worldcubeassociation.org/results as of March 12, 2025.\n\n\n\n\nThe WCA database was originally created and maintained by:\n\nClément Gallet, France\nStefan Pochmann, Germany\nJosef Jelinek, Czech Republic\nRon van Bruchem, Netherlands\n\nThe database contents are now maintained by the WCA Results Team, and the software for the database is maintained by the WCA Software Team: https://www.worldcubeassociation.org/about\n\n\n\nThe export contains a metadata.json file, with the following fields:\n\n\n\nField\nSample Value\n\n\n\n\nexport_date\n\"2025-03-12 00:02:02 UTC\"\n\n\nexport_format_version\n\"1.0.0\"\n\n\n\nIf you regularly process this export, we recommend that you check the export_format_version value in your program and and review your code if the major part of the version (the part before the first .) changes.\nIf you are processing the exported data using an automated system, we recommend using a cron job to check the API endpoint at: https://www.worldcubeassociation.org/api/v0/export/public You can use the export_date to detect if there is a new export, and the sql_url and tsv_url will contain the URLs for the corresponding downloads.\n\n\n\nThe database export consists of these tables:\n\n\n\n\n\n\n\nTable\nContents\n\n\n\n\nPersons\nWCA competitors\n\n\nCompetitions\nWCA competitions\n\n\nEvents\nWCA events (3x3x3 Cube, Megaminx, etc)\n\n\nResults\nWCA results per competition+event+round+person\n\n\nRanksSingle\nBest single result per competitor+event and ranks\n\n\nRanksAverage\nBest average result per competitor+event and ranks\n\n\nRoundTypes\nThe round types (first, final, etc)\n\n\nFormats\nThe round formats (best of 3, average of 5, etc)\n\n\nCountries\nCountries\n\n\nContinents\nContinents\n\n\nScrambles\nScrambles\n\n\nchampionships\nChampionship competitions\n\n\neligible_country_iso2s_for_championship\nSee explanation below\n\n\n\nMost of the tables should be self-explanatory, but here are a few specific details:\n\n\nCountries stores include those from the Wikipedia list of countries at http://en.wikipedia.org/wiki/List_of_countries, and may include some countries that no longer exist. The ISO2 column should reflect ISO 3166-1 alpha-2 country codes, for countries that have them. Custom codes may be used in some circumstances.\n\n\n\nScrambles stores all scrambles.\nFor 333mbf, an attempt is comprised of multiple newline-separated scrambles. However, newlines can cause compatibility issues with TSV parsers. Therefore, in the TSV version of the data we replace each newline in a 333mbf scramble with the | character.\n\n\n\neligible_country_iso2s_for_championship stores information about which citizenships are eligible to win special cross-country championship types.\nFor example, greater_china is a special championship type which contains 4 iso2 values: CN, HK, MC and TW. This means that any competitor from China, Hong Kong, Macau, or Taiwan is eligible to win a competition with championship type greater_china.\n\n\n\nPlease see https://www.worldcubeassociation.org/regulations/#article-9-events for information about how results are measured.\nValues of the Results table can be interpreted as follows:\n\nThe result values are in the following fields value1, value2, value3, value4, value5, best, and average.\nThe value -1 means DNF (Did Not Finish).\nThe value -2 means DNS (Did Not Start).\nThe value 0 means “no result”. For example a result in a best-of-3 round has a value of 0 for the value4, value5, and average fields.\nPositive values depend on the event; see the column “format” in Events.\n\nMost events have the format “time”, where the value represents centiseconds. For example, 8653 means 1 minute and 26.53 seconds.\nThe format “number” means the value is a raw number, currently only used by “fewest moves” for number of moves.\n\nFewest moves averages are stored as 100 times the average, rounded.\n\nThe format “multi” is for old and new multi-blind, encoding the time as well as the number of cubes attempted and solved. This is a decimal value, which can be interpreted (“decoded”) as follows:\nold: 1SSAATTTTT\n     solved        = 99 - SS\n     attempted     = AA\n     timeInSeconds = TTTTT (99999 means unknown)\nnew: 0DDTTTTTMM\n     difference    = 99 - DD\n     timeInSeconds = TTTTT (99999 means unknown)\n     missed        = MM\n     solved        = difference + missed\n     attempted     = solved + missed\nIn order to encode data, use the following procedure:\n     solved        = # cubes solved\n     attempted     = # cubes attempted\n     missed        = # cubes missed = attempted - solved\n     DD            = 99 - (solved - missed)\n     TTTTT         = solve time in seconds\n     MM            = missed\nNote that this is designed so that a smaller decimal value means a better result. This format does not support more than 99 attempted cubes, or times greater than 99999 seconds (about 27.7 hours)."
  },
  {
    "objectID": "WCA_data/WAC_README.html#description",
    "href": "WCA_data/WAC_README.html#description",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "This database export contains public information about all official WCA competitions, WCA members, and WCA competition results."
  },
  {
    "objectID": "WCA_data/WAC_README.html#goal",
    "href": "WCA_data/WAC_README.html#goal",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The goal of this database export is to provide members of the speedcubing community a practical way to perform analysis on competition information for statistical and personal purposes."
  },
  {
    "objectID": "WCA_data/WAC_README.html#allowed-use",
    "href": "WCA_data/WAC_README.html#allowed-use",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The information in this file may be re-published, in whole or in part, as long as users are clearly notified of the following:\n\nThis information is based on competition results owned and maintained by the World Cube Assocation, published at https://worldcubeassociation.org/results as of March 12, 2025."
  },
  {
    "objectID": "WCA_data/WAC_README.html#acknowledgements",
    "href": "WCA_data/WAC_README.html#acknowledgements",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The WCA database was originally created and maintained by:\n\nClément Gallet, France\nStefan Pochmann, Germany\nJosef Jelinek, Czech Republic\nRon van Bruchem, Netherlands\n\nThe database contents are now maintained by the WCA Results Team, and the software for the database is maintained by the WCA Software Team: https://www.worldcubeassociation.org/about"
  },
  {
    "objectID": "WCA_data/WAC_README.html#date-and-format-version",
    "href": "WCA_data/WAC_README.html#date-and-format-version",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The export contains a metadata.json file, with the following fields:\n\n\n\nField\nSample Value\n\n\n\n\nexport_date\n\"2025-03-12 00:02:02 UTC\"\n\n\nexport_format_version\n\"1.0.0\"\n\n\n\nIf you regularly process this export, we recommend that you check the export_format_version value in your program and and review your code if the major part of the version (the part before the first .) changes.\nIf you are processing the exported data using an automated system, we recommend using a cron job to check the API endpoint at: https://www.worldcubeassociation.org/api/v0/export/public You can use the export_date to detect if there is a new export, and the sql_url and tsv_url will contain the URLs for the corresponding downloads."
  },
  {
    "objectID": "WCA_data/WAC_README.html#format-version-1.0.0",
    "href": "WCA_data/WAC_README.html#format-version-1.0.0",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The database export consists of these tables:\n\n\n\n\n\n\n\nTable\nContents\n\n\n\n\nPersons\nWCA competitors\n\n\nCompetitions\nWCA competitions\n\n\nEvents\nWCA events (3x3x3 Cube, Megaminx, etc)\n\n\nResults\nWCA results per competition+event+round+person\n\n\nRanksSingle\nBest single result per competitor+event and ranks\n\n\nRanksAverage\nBest average result per competitor+event and ranks\n\n\nRoundTypes\nThe round types (first, final, etc)\n\n\nFormats\nThe round formats (best of 3, average of 5, etc)\n\n\nCountries\nCountries\n\n\nContinents\nContinents\n\n\nScrambles\nScrambles\n\n\nchampionships\nChampionship competitions\n\n\neligible_country_iso2s_for_championship\nSee explanation below\n\n\n\nMost of the tables should be self-explanatory, but here are a few specific details:\n\n\nCountries stores include those from the Wikipedia list of countries at http://en.wikipedia.org/wiki/List_of_countries, and may include some countries that no longer exist. The ISO2 column should reflect ISO 3166-1 alpha-2 country codes, for countries that have them. Custom codes may be used in some circumstances.\n\n\n\nScrambles stores all scrambles.\nFor 333mbf, an attempt is comprised of multiple newline-separated scrambles. However, newlines can cause compatibility issues with TSV parsers. Therefore, in the TSV version of the data we replace each newline in a 333mbf scramble with the | character.\n\n\n\neligible_country_iso2s_for_championship stores information about which citizenships are eligible to win special cross-country championship types.\nFor example, greater_china is a special championship type which contains 4 iso2 values: CN, HK, MC and TW. This means that any competitor from China, Hong Kong, Macau, or Taiwan is eligible to win a competition with championship type greater_china.\n\n\n\nPlease see https://www.worldcubeassociation.org/regulations/#article-9-events for information about how results are measured.\nValues of the Results table can be interpreted as follows:\n\nThe result values are in the following fields value1, value2, value3, value4, value5, best, and average.\nThe value -1 means DNF (Did Not Finish).\nThe value -2 means DNS (Did Not Start).\nThe value 0 means “no result”. For example a result in a best-of-3 round has a value of 0 for the value4, value5, and average fields.\nPositive values depend on the event; see the column “format” in Events.\n\nMost events have the format “time”, where the value represents centiseconds. For example, 8653 means 1 minute and 26.53 seconds.\nThe format “number” means the value is a raw number, currently only used by “fewest moves” for number of moves.\n\nFewest moves averages are stored as 100 times the average, rounded.\n\nThe format “multi” is for old and new multi-blind, encoding the time as well as the number of cubes attempted and solved. This is a decimal value, which can be interpreted (“decoded”) as follows:\nold: 1SSAATTTTT\n     solved        = 99 - SS\n     attempted     = AA\n     timeInSeconds = TTTTT (99999 means unknown)\nnew: 0DDTTTTTMM\n     difference    = 99 - DD\n     timeInSeconds = TTTTT (99999 means unknown)\n     missed        = MM\n     solved        = difference + missed\n     attempted     = solved + missed\nIn order to encode data, use the following procedure:\n     solved        = # cubes solved\n     attempted     = # cubes attempted\n     missed        = # cubes missed = attempted - solved\n     DD            = 99 - (solved - missed)\n     TTTTT         = solve time in seconds\n     MM            = missed\nNote that this is designed so that a smaller decimal value means a better result. This format does not support more than 99 attempted cubes, or times greater than 99999 seconds (about 27.7 hours)."
  },
  {
    "objectID": "data_for_mapping.html",
    "href": "data_for_mapping.html",
    "title": "Ideas for Mapping Data",
    "section": "",
    "text": "There is a lot of data in the world, pick a set that you are comfortable with by the end of the first project day. You can go down a rabbit hole after the semester is over."
  },
  {
    "objectID": "data_for_mapping.html#caution",
    "href": "data_for_mapping.html#caution",
    "title": "Ideas for Mapping Data",
    "section": "",
    "text": "There is a lot of data in the world, pick a set that you are comfortable with by the end of the first project day. You can go down a rabbit hole after the semester is over."
  },
  {
    "objectID": "data_for_mapping.html#map-data-ideas-for-project-3",
    "href": "data_for_mapping.html#map-data-ideas-for-project-3",
    "title": "Ideas for Mapping Data",
    "section": "Map data ideas for project 3",
    "text": "Map data ideas for project 3\n\nEPA Toxic Release Inventory (TRI)\nhttps://www.epa.gov/toxics-release-inventory-tri-program/tri-basic-data-files-calendar-years-1987-present\n\n\nCDC Flu View Interactive.\nhttps://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n\n\nMassachusetts GIS Data\nhttps://www.mass.gov/info-details/massgis-data-layers\nOther states also have GIS Data\n\n\nSearch Geo Spatial Analysis on Kaggle\nhttps://www.kaggle.com/datasets?sort=usability&tags=13206-Geospatial+Analysis\n(Sort by usability)"
  },
  {
    "objectID": "scorecard_analysis.html",
    "href": "scorecard_analysis.html",
    "title": "College Statistics",
    "section": "",
    "text": "Plot 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post\nThe data we are working with for our research and findings is the College Scorecard Institution-Level Data. This dataset provides information about institutions as a whole and also specific fields of study in the United States. This data was produced by federal reports by institutions, financial aid data, and tax data. Specifically, we are focusing on post secondary institutions in Massachusetts in 2021. Our guiding question is in Massachusetts in 2021, are private, non profit, or public schools thriving in terms of student popularity? To investigate this broad topic, we look at the data from private, non profit, and public post secondary schools data including their number of undergraduate students and their admissions rate.\nOur first plot looks at the relationship between average number of students and type of institution. The graph is split into three separate bars for each institution type: public, private non-profit, and private for-profit. We can conclude from this graph that typically the public institutions in Massachusetts have a higher number of students with an average of 3480 students per institution. The outlier is the Private For-Profit average number of students with 151 students. This number is brought down by the smaller specialized schools such as Empire Beauty School-Boston with only 87 people in the undergraduate population. The public universities in this graph don’t show a great discrepancy between the public and private non-profit institutions but in reality, there is such a large gap between the largest and smallest number of students in all of the public universities. The average is brought down by all of the smaller public schools. Our second plot looks at the relationship between admissions rate and number of undergraduate students, colored by the status of private, non profit, and public institutions. From this graph, we observe that for public institutions, the vast majority have a very high admission rate but hover around an average number of undergraduate students enrolled. This implies that public colleges/universities are relatively popular, as they admit a large number of students while having an average number of undergraduates enrolled. For private institutions, they are evenly spread along the admission rate, while remaining comparable numbers of undergraduate enrollment. We conclude from this information that since these colleges and universities have a similar number of undergraduates as public institutions, but often lower admission rates, they are more popular than public institutions.\nThe key takeaway from our analysis is that public institutions have larger student populations than private or nonprofit institutions, indicating a level of popularity, but have higher admission rates, indicating a possibility of less popularity in terms of applicant pool. The data collected for this dataset is those institutions that receive federal funding. The institutions collect their own data, and may have different ways of doing so, possibly leading to different reports and non-standardized data collection. This data also provides financial information in terms of the institution to families and applicants, but not the culture or surroundings of the college itself, including city or town living expenses."
  },
  {
    "objectID": "WCA_analysis.html",
    "href": "WCA_analysis.html",
    "title": "World Cube Association",
    "section": "",
    "text": "Plot 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post\nThe dataset we are working with contains information about Rubik’s Cube competitions from across the world. These data were recorded and published by the World Cube Association. The data includes information about the location of the competitions, the ranks of the competitors at different levels, the home countries of the competitors, and the gender of the competitors. We were specifically looking at competitions using a standard 3 by 3 Rubik’s Cube. We were interested in exploring the breakdown of gender for competitors from each continent, and the total number of competitors from each continent. Originally, we attempted to look at a breakdown of competitors by country, but these data contain information about competitors from so many countries that it became difficult to look at in a graph form. Thus, we decided to change our question to look at gender by continent instead. This required some data wrangling to match each country to its corresponding continent. The wrangling resulted in a bar graph showing the breakdown of competitors for each continent, colored based on gender. Asia had the most competitors (over 80,000), then North America, then Europe, then South America, then Oceania, and finally Africa (under 5,000). From the bar graph it was immediately apparent that there were more male competitors than female competitors. This was true for each continent. We then decided to display the data in another way, so we made two pie charts. The first pie chart shows the breakdown of which continent the female competitors came from (in blues). The second pie chart shows the breakdown of which continent different male competitors came from (in reds). Key takeaways from our data wrangling and plotting show that across all continents more male competitors attend Rubik’s competitions, and more competitors come from Asia than any other continent, both male and female. When we join datasets we need to consider what new information can be gleaned from combining the data. For example, if we have one dataset with people’s names and how much money they spend on milk, and another set with people’s names and addresses, combining those datasets could lead to targeted ad campaigns, which would not be possible with either one of the datasets by itself. In the case of this dataset, you could use the date and address of the competitions, combined with the names of the competitors attending to find where and when a person was in a location – which could potentially be an invasion of privacy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leah Levensailor",
    "section": "",
    "text": "I am currently studying at Smith College with a double major in Computer Science and Statisical Data Science. I have known for a long time I wanted to work in some sort of Computer Science field, using technology to enhance the human experience and help those who are underprivileged and overlooked. Whether that be working on technology for amputees or speech devices for children with developmental disabilities I have always wanted to make a difference using the skills I have.\nOn top of that, my love for Data Science and Statistics came as a surprise after high school when I couldn’t seem to escape the little voice in my head telling me to take these classes. Along the same vein, I want to use my knowledge to help people and give people information and statistics that can truly better their lives. My work with the Vascular Research Lab at Smith College is helping me achieve that goal. When I am not doing coursework, you can typically find me building a new LEGO set or learning a new skyle such as crocheting. Anything to keep my hands busy and my brain engaged.\n\n\n\nBA in Computer Science and Statistical Data Science (~2028)\nSmith College\nNorthampton, Massachusetts\n\n\n\n\n\nSDS 192: Intro to Data Science\nSDS 100: Lab Computing with Data\nCSC 120: Object Oriented Programming\nCSC 210: Data Structures\nCSC 231: Microprocessors and Assembly Language\nSDS 291: Multiple Regression\nSDS 290: Research Design and Analysis\nSDS 235: Visual Analytics\nCSC 240: Computer Graphics\nCSC 250: Theoretical Foundations\nemail: leahlevi1018@gmail.com\nstudent email: llevensailor@smith.edu"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Leah Levensailor",
    "section": "",
    "text": "BA in Computer Science and Statistical Data Science (~2028)\nSmith College\nNorthampton, Massachusetts"
  },
  {
    "objectID": "index.html#related-courses",
    "href": "index.html#related-courses",
    "title": "Leah Levensailor",
    "section": "",
    "text": "SDS 192: Intro to Data Science\nSDS 100: Lab Computing with Data\nCSC 120: Object Oriented Programming\nCSC 210: Data Structures\nCSC 231: Microprocessors and Assembly Language\nSDS 291: Multiple Regression\nSDS 290: Research Design and Analysis\nSDS 235: Visual Analytics\nCSC 240: Computer Graphics\nCSC 250: Theoretical Foundations"
  },
  {
    "objectID": "CaseStudy1.html",
    "href": "CaseStudy1.html",
    "title": "Voter Mistakes Take Election",
    "section": "",
    "text": "The US presidential election of 2000 has been brought to our attention because of how close the voter margin was. The votes were so close that Democratic nominee Al Gore had both conceded and taken back his concession all in one night. At the end of the day a recount was needed because of the close margin. While the recount was underway, the voters of Palm Beach County, Florida were upset about the confusing ballot operation this election. The ballot supposedly tripped people up thinking they were voting for Gore when they were actually voting for the reform candidate Pat Buchanan.\n\nThe question we are trying to answer is is there sufficient evidence that people were voting for Buchanan thinking they were actually voting for Gore. This came up because of the unusually high number of votes Buchanan got in that specific county. Below shows the tests I’ve run to see if this claim is true. The data we have is a collection of the number of votes for Bush and Buchanan in all of the counties in Florida.\n\nlibrary(tidyverse)\nlibrary(broom)\n# Reading in and saving the data\nelection &lt;- Sleuth2::ex0825\n\n# Creating a second dataset with Palm Beach County excluded\nelection_wo_pb &lt;- election |&gt;\n  filter(County != \"Palm Beach\")\n\n\n\nThe data in its original form is not linear and is clumped towards the bottom left corner. Because of this I performed a logarithmic transformation on the explanatory and reposne variables. This resulted in a drastically more linear scatterplot.\n\n#transforming the data by logs \ntransformed_bush &lt;- log(election_wo_pb$Bush2000)\ntransformed_buc &lt;- log(election_wo_pb$Buchanan2000)\n\n#mutating the data frame to include these new values \nt_elections_wo_pb &lt;- election_wo_pb |&gt;\n  mutate(Bush2000 = transformed_bush) |&gt;\n  mutate(Buchanan2000 = transformed_buc)\n\n\n#plotting the newly transformed data\nt_elections_wo_pb |&gt;\n  ggplot(aes(x = Bush2000, y = Buchanan2000)) +\n  geom_point(size = 2) +\n  scale_x_continuous(labels = scales::label_comma()) +\n  xlab(\"Number of Votes for Bush\") +\n  ylab(\"Number of Votes for Buchanan\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n#creating the data for the residuals \nmod_election &lt;- lm(Buchanan2000 ~ Bush2000, data = t_elections_wo_pb)\nres_election &lt;- resid(mod_election)\nplot(fitted(mod_election), res_election) \nabline(0,0)\n\n\n\n\n\n\n\n\n\n#residual plots \nqqnorm(res_election)\nqqline(res_election)\n\n\n\n\n\n\n\n\nUsing the residual plots and scatter plots from above I can confirm that the Linear, Independent, Normal and Equal Variance conditions are sufficiently met. The model I have come up with is \\[\\log(\\widehat{Buchanan}) = \\beta_0 + \\beta_1\\log(Bush)\\] where log(Buchanan) is the natural logarithm of the number of votes for Buchanan in the individual county and log(Bush) is the natural logarithm of the number of votes for Bush in the individual county. \\(\\beta_0\\) in this context represents when Bush get’s zero votes in a county, Buchanan is expected to get -2.341 votes. \\(\\beta_1\\) in this context represents a one vote increase in Bush votes leads to an expected 0.731 increase in Buchanan votes on average for each individual county. The equation I have come up to predict Buchanan votes from Bush votes is \\[\\log(\\widehat{Buchanan}) = -2.341 + 0.731\\log(Bush)\\]\n\n\n\nThe question is asking for a 95% prediction interval using the fitted model which is what I have done below.\n\n#95% prediction interval \nnew_election &lt;- data.frame(Bush2000 = 5.18425)\n\nprediction_intervals &lt;- mod_election |&gt;\n  augment(\n    newdata = new_election,\n    interval = \"prediction\",\n    conf.level = 0.95\n  )\n\nSince I had to transform the data, the interval \\((0.542, 2.35)\\) is still in log form and we have to change it back to show the real interval.\n\nexp(0.542)\n\n[1] 1.719442\n\nexp(2.35)\n\n[1] 10.48557\n\nexp(5.18425)\n\n[1] 178.4396\n\nexp(-2.341 +(0.731*exp(5.18425)))\n\n[1] 4.289394e+55\n\n\n\\[e^{0.542} = 1.72\\]\n\\[\ne^{2.35} = 10.48\n\\]"
  },
  {
    "objectID": "CaseStudy1.html#introduction",
    "href": "CaseStudy1.html#introduction",
    "title": "Voter Mistakes Take Election",
    "section": "",
    "text": "The US presidential election of 2000 has been brought to our attention because of how close the voter margin was. The votes were so close that Democratic nominee Al Gore had both conceded and taken back his concession all in one night. At the end of the day a recount was needed because of the close margin. While the recount was underway, the voters of Palm Beach County, Florida were upset about the confusing ballot operation this election. The ballot supposedly tripped people up thinking they were voting for Gore when they were actually voting for the reform candidate Pat Buchanan.\n\nThe question we are trying to answer is is there sufficient evidence that people were voting for Buchanan thinking they were actually voting for Gore. This came up because of the unusually high number of votes Buchanan got in that specific county. Below shows the tests I’ve run to see if this claim is true. The data we have is a collection of the number of votes for Bush and Buchanan in all of the counties in Florida.\n\nlibrary(tidyverse)\nlibrary(broom)\n# Reading in and saving the data\nelection &lt;- Sleuth2::ex0825\n\n# Creating a second dataset with Palm Beach County excluded\nelection_wo_pb &lt;- election |&gt;\n  filter(County != \"Palm Beach\")\n\n\n\nThe data in its original form is not linear and is clumped towards the bottom left corner. Because of this I performed a logarithmic transformation on the explanatory and reposne variables. This resulted in a drastically more linear scatterplot.\n\n#transforming the data by logs \ntransformed_bush &lt;- log(election_wo_pb$Bush2000)\ntransformed_buc &lt;- log(election_wo_pb$Buchanan2000)\n\n#mutating the data frame to include these new values \nt_elections_wo_pb &lt;- election_wo_pb |&gt;\n  mutate(Bush2000 = transformed_bush) |&gt;\n  mutate(Buchanan2000 = transformed_buc)\n\n\n#plotting the newly transformed data\nt_elections_wo_pb |&gt;\n  ggplot(aes(x = Bush2000, y = Buchanan2000)) +\n  geom_point(size = 2) +\n  scale_x_continuous(labels = scales::label_comma()) +\n  xlab(\"Number of Votes for Bush\") +\n  ylab(\"Number of Votes for Buchanan\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n#creating the data for the residuals \nmod_election &lt;- lm(Buchanan2000 ~ Bush2000, data = t_elections_wo_pb)\nres_election &lt;- resid(mod_election)\nplot(fitted(mod_election), res_election) \nabline(0,0)\n\n\n\n\n\n\n\n\n\n#residual plots \nqqnorm(res_election)\nqqline(res_election)\n\n\n\n\n\n\n\n\nUsing the residual plots and scatter plots from above I can confirm that the Linear, Independent, Normal and Equal Variance conditions are sufficiently met. The model I have come up with is \\[\\log(\\widehat{Buchanan}) = \\beta_0 + \\beta_1\\log(Bush)\\] where log(Buchanan) is the natural logarithm of the number of votes for Buchanan in the individual county and log(Bush) is the natural logarithm of the number of votes for Bush in the individual county. \\(\\beta_0\\) in this context represents when Bush get’s zero votes in a county, Buchanan is expected to get -2.341 votes. \\(\\beta_1\\) in this context represents a one vote increase in Bush votes leads to an expected 0.731 increase in Buchanan votes on average for each individual county. The equation I have come up to predict Buchanan votes from Bush votes is \\[\\log(\\widehat{Buchanan}) = -2.341 + 0.731\\log(Bush)\\]\n\n\n\nThe question is asking for a 95% prediction interval using the fitted model which is what I have done below.\n\n#95% prediction interval \nnew_election &lt;- data.frame(Bush2000 = 5.18425)\n\nprediction_intervals &lt;- mod_election |&gt;\n  augment(\n    newdata = new_election,\n    interval = \"prediction\",\n    conf.level = 0.95\n  )\n\nSince I had to transform the data, the interval \\((0.542, 2.35)\\) is still in log form and we have to change it back to show the real interval.\n\nexp(0.542)\n\n[1] 1.719442\n\nexp(2.35)\n\n[1] 10.48557\n\nexp(5.18425)\n\n[1] 178.4396\n\nexp(-2.341 +(0.731*exp(5.18425)))\n\n[1] 4.289394e+55\n\n\n\\[e^{0.542} = 1.72\\]\n\\[\ne^{2.35} = 10.48\n\\]"
  },
  {
    "objectID": "CaseStudy1.html#conclusion",
    "href": "CaseStudy1.html#conclusion",
    "title": "Voter Mistakes Take Election",
    "section": "Conclusion",
    "text": "Conclusion\nWe are 95% confident that the number of votes for Buchanan in Palm Beach is within the interval 1.72 and 10.48 votes. In reality in Palm Beach, Buchanan got 3407 votes which tells us that approximately 3396 to 3405 of those votes were supposed to go to Gore according to our prediction interval. These findings are statistically significant and that we can reject the null hypothesis that there was no association with the type of ballot in Palm Beach County and the number of votes Buchanan got. The limitations on my conclusions are they only apply to Palm Beach County Florida and cannot be generalized to the rest of the country. We would have to complete this test with every state. The other part of it is these ballot complaints were just from that one specific county. It is unclear for the rest of the country what their ballots looked like and if each person voted for who they thought they were voting for. This test also cannot guarantee that Bush or Gore should have won, it was only to figure out if that ballot caused issues. We did find out that Buchanan received a significantly larger number of votes than he was supposed to since the number he got was not in the prediction interval.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.341486\n0.3544151\n-6.606621\n0\n\n\nBush2000\n0.730962\n0.0359673\n20.322942\n0"
  },
  {
    "objectID": "CaseStudy2.html",
    "href": "CaseStudy2.html",
    "title": "Racial Inequality in Wages",
    "section": "",
    "text": "A persistent racial disparity within the American labor market is the wage gap between Black and white individuals. This study analyses data from 25,631 male respondents in the March 1988 Current Population Survey (CPS), examining whether Black men continue to earn less than non-Black men with comparable qualifications after controlling for factors such as years of education, work experience, and region.\nThe United States Census Bureau hypothesizes that wage disparities among Black workers may vary by region. This study will:\n\nEstablish a model that allows racial effects to vary across regions while adjusting the influence of education and work experience;\nEstimate the wage gap between Black and non-Black workers in each region;\nConduct hypothesis tests to determine whether these regional wage gaps are statistically significant;\nUltimately answer whether Black men indeed earn less after controlling for region, education, and work experience."
  },
  {
    "objectID": "CaseStudy2.html#constructing-and-fitting-the-model",
    "href": "CaseStudy2.html#constructing-and-fitting-the-model",
    "title": "Racial Inequality in Wages",
    "section": "Constructing and Fitting the Model",
    "text": "Constructing and Fitting the Model\nWe will construct a multiple linear regression model with the natural logarithm of weekly wage as the response variable and years of education, years of experience, race, metropolitan status, and geographic region as explanatory variables.\n\n# Construct a model with log-transformed response variable\nmodel_interaction_log &lt;- lm(\n  log(Wage) ~ Black * Region + Education + Experience,\n  data = wages\n)\n\nLet \\(\\text{Wage}_i\\) denote the wage of individual \\(i\\). Our final log-linear regression model is:\n\\[\nE[\\log(\\text{Wage}_i) \\mid X_i] =\n\\beta_0 + \\beta_1 \\text{Black}_i + \\beta_2 \\text{RegionNE}_i\n+ \\beta_3 \\text{RegionS}_i + \\beta_4 \\text{RegionW}_i\n\\]\n\\[\n+ \\beta_5 \\text{Education}_i + \\beta_6 \\text{Experience}_i\n+ \\beta_7 (\\text{Black}_i \\times \\text{RegionNE}_i)\n+ \\beta_8 (\\text{Black}_i \\times \\text{RegionS}_i)\n+ \\beta_9 (\\text{Black}_i \\times \\text{RegionW}_i).\n\\]\nThe coefficient \\(\\beta_1\\) represents the multiplicative effect of being Black on the median wage, holding all other variables fixed.\nInteraction terms imply that the effect of being Black may differ across regions, with the multiplicative effect in region \\(k\\) given by \\(\\exp(\\beta_1 + \\beta_k)\\), where \\(\\beta_k\\) is the corresponding interaction coefficient.\nThe estimation of the parameters is shown below in the table.\n\n# Converting model output to a tidy dataframe\ninteraction_lm_table &lt;- model_interaction_log |&gt;\n  broom::tidy()\n\n# Creating a nicely formatted regression table using kable\ninteraction_lm_table |&gt;\n  knitr::kable(\n    digits = 4\n  )\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.6601\n0.0195\n239.2107\n0.0000\n\n\nBlackYes\n-0.1928\n0.0307\n-6.2866\n0.0000\n\n\nRegionNE\n0.0607\n0.0100\n6.0785\n0.0000\n\n\nRegionS\n-0.0548\n0.0095\n-5.7507\n0.0000\n\n\nRegionW\n0.0034\n0.0101\n0.3330\n0.7391\n\n\nEducation\n0.0989\n0.0012\n81.9261\n0.0000\n\n\nExperience\n0.0183\n0.0003\n64.9916\n0.0000\n\n\nBlackYes:RegionNE\n-0.0035\n0.0432\n-0.0817\n0.9349\n\n\nBlackYes:RegionS\n-0.0418\n0.0350\n-1.1916\n0.2334\n\n\nBlackYes:RegionW\n0.0382\n0.0514\n0.7429\n0.4575"
  },
  {
    "objectID": "CaseStudy2.html#assessing-conditions-for-log-transformed-model",
    "href": "CaseStudy2.html#assessing-conditions-for-log-transformed-model",
    "title": "Racial Inequality in Wages",
    "section": "Assessing Conditions For Log-transformed Model",
    "text": "Assessing Conditions For Log-transformed Model\n\n# Check condition for log-transformed model\nplot(model_interaction_log)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the log-transformed model shows some improvement over the original scale model, it still exhibits some limitations. The residual plot reveals some heteroscedasticity, with variance first increases then decreases when the fitting value y is larger, and the Q-Q plot shows slight deviations from normality in the tails. However, these departures are less severe than in the original model, where clear heteroscedasticity and extreme non-normality violated core regression assumptions. Therefore, we will continue to use the log-transformed model in our study."
  },
  {
    "objectID": "CaseStudy2.html#anova-and-results",
    "href": "CaseStudy2.html#anova-and-results",
    "title": "Racial Inequality in Wages",
    "section": "ANOVA and Results",
    "text": "ANOVA and Results\n\nwages |&gt;\n  ggplot(aes(x = Education, y = log(Wage), color = Black)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  ggtitle(\"Education and log-transformed wages by race\")\n\n\n\n\n\n\n\n\nThe graph above shows how Education and Wage relate with respect to race. The explanatory variable here is Education\n\n# Tidy and format output\nmodel_interaction_log |&gt;\n  tidy() |&gt;\n  kable(digits = c(NA, 3, 3, 2, 4))\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.660\n0.019\n239.21\n0.0000\n\n\nBlackYes\n-0.193\n0.031\n-6.29\n0.0000\n\n\nRegionNE\n0.061\n0.010\n6.08\n0.0000\n\n\nRegionS\n-0.055\n0.010\n-5.75\n0.0000\n\n\nRegionW\n0.003\n0.010\n0.33\n0.7391\n\n\nEducation\n0.099\n0.001\n81.93\n0.0000\n\n\nExperience\n0.018\n0.000\n64.99\n0.0000\n\n\nBlackYes:RegionNE\n-0.004\n0.043\n-0.08\n0.9349\n\n\nBlackYes:RegionS\n-0.042\n0.035\n-1.19\n0.2334\n\n\nBlackYes:RegionW\n0.038\n0.051\n0.74\n0.4575\n\n\n\n\n# Test if pay gaps differ by region\nanova(model_interaction_log)\n\nAnalysis of Variance Table\n\nResponse: log(Wage)\n                Df Sum Sq Mean Sq   F value Pr(&gt;F)    \nBlack            1  170.7  170.73  594.9801 &lt;2e-16 ***\nRegion           3   91.8   30.60  106.6532 &lt;2e-16 ***\nEducation        1 1257.7 1257.72 4382.9684 &lt;2e-16 ***\nExperience       1 1213.2 1213.23 4227.9287 &lt;2e-16 ***\nBlack:Region     3    1.2    0.42    1.4489 0.2264    \nResiduals    25621 7352.1    0.29                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooking at the ANOVA test we can see that each of th explanatory variables are statistically significant except for the interaction between if the person identified as Black and the region. Since the ANOVA showed there was no statistical relationship we now want to look at the actual wage difference for each reagion and race holding for education and experience.\n\n# Predict wages for Black and non-Black workers in each region\npredict_df &lt;- expand.grid(\n  Black = levels(wages$Black),\n  Region = levels(wages$Region),\n  Education = mean(wages$Education, na.rm = TRUE),\n  Experience = mean(wages$Experience, na.rm = TRUE)\n)\n\npredict_df$logWage &lt;- predict(model_interaction_log, newdata = predict_df)\npredict_df$Wage &lt;- exp(predict_df$logWage)\n\n# View region-specific wage estimates\npredict_df\n\n  Black Region Education Experience  logWage     Wage\n1    No     MW  13.07627   18.58656 6.293260 540.9137\n2   Yes     MW  13.07627   18.58656 6.100459 446.0625\n3    No     NE  13.07627   18.58656 6.353951 574.7593\n4   Yes     NE  13.07627   18.58656 6.157623 472.3041\n5    No      S  13.07627   18.58656 6.238463 512.0709\n6   Yes      S  13.07627   18.58656 6.003899 405.0048\n7    No      W  13.07627   18.58656 6.296639 542.7445\n8   Yes      W  13.07627   18.58656 6.142045 465.0036\n\n\n\nggplot(predict_df, aes(x = Region, y = Wage, fill = Black)) + \n  geom_col(position = \"dodge\") +\n  labs(title = \"Wage per Region Separated by Race\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs shown in the figure above, people who identify as Black consistently have a lower wage than those who do not identify as Black. However there is not a statistically significant enough gap between the wages per region to say that the region someone is in has an effect on their wage. The data has backed up the fact that there is a wage gap but we cannot say that there is enough evidence based on our study that region affects wage for people who identify as Black."
  },
  {
    "objectID": "CaseStudy2.html#summary",
    "href": "CaseStudy2.html#summary",
    "title": "Racial Inequality in Wages",
    "section": "Summary",
    "text": "Summary\nWe fit a multiple linear regression model to predict the log of hourly wages using race, region, education, and experience, including an interaction between race and region to test the U.S. Census Bureau’s hypothesis that racial pay disparities vary geographically. The ANOVA results showed that the interaction term was not statistically significant (F = 1.45, p = 0.2264), indicating that the wage gap between Black and non-Black workers does not differ meaningfully across regions. Consequently, we simplified the model to include only main effects. All main predictors (race, region, education, and experience) were highly significant (p &lt; 0.001), with education and experience strongly associated with higher wages, and race (Black) associated with lower wages even after adjusting for other factors. Using the interaction model, we predicted wages for Black and non-Black workers in each region, holding education and experience constant, and found a consistent pay gap across regions, with Black workers earning less than their non-Black counterparts. These findings confirm that Black males were paid less than non-Black males in the same region with equivalent education and experience, and that this disparity is statistically significant and geographically consistent."
  }
]